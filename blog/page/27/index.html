
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>liwanag</title>
  <meta name="author" content="nadine a.">

  
  <meta name="description" content="Operating Systems Notes [Chapter 5: CPU Scheduling] Jan 18th, 2011 SchedulingRecap: multiprogramming takes advantage of the fact that no one process &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.neauro.com/blog/page/27">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="liwanag" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<!-- <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css"> -->
<link href="http://fonts.googleapis.com/css?family=Metrophobic:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-46467190-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body    class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">liwanag</a></h1>
  
    <h2>things i learn, things i like</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:blog.neauro.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/about">About</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/18/operating-systems-notes-chapter-5-cpu/">Operating Systems Notes [Chapter 5: CPU Scheduling]</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-18T00:00:00-08:00" pubdate data-updated="true">Jan 18<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<span class="sig">Scheduling</span><br /><a href="http://nuubu.blogspot.com/2011/01/operating-system-notes-chapter-1.html">Recap</a>: <strong>multiprogramming</strong> takes advantage of the fact that no one process can keep the CPU busy at all times, so the CPU switches between different jobs so that it&#8217;s always busy executing something.  This makes the computer more productive in general.<br /><br />A process is comprised of an address space, OS resources, and <em>at least one thread of execution</em>, and it&#8217;s threads that get scheduled.  The operating system specifically schedules kernel-threads, which are threads of execution for programs that are part of the kernel.<br /><br />Scheduling is what a scheduler does.  Scheduling is concerned with two things:<ol><li>How long will the current process run on the CPU?</li><li>What process will get the CPU next?</li></ol><br />Actually, resources other than the CPU can be scheduled, like threads or devices or I/O access or pretty much everything else in a computer.  But, for now, just the CPU.<br /><br />Some challenges to scheduler design are<ul><li>There are new jobs all the time</li><li>How can you schedule a process when you&#8217;re not sure how long it will take to run completely?</li></ul><br /><br /><span class="sig">Process Cycles</span><br />Essential to scheduling is the idea that processes execute in a <strong>cycle</strong>, which is comprised of a <strong>CPU burst</strong> and an <strong>I/O burst</strong>, and then another CPU burst, and then another I/O burst, and so on until execution is terminated.<br /><br />During the CPU burst, the process is doing things like loading memory, incrementing variables, reading from file, writing to file, etc.  During the I/O burst, the process is just waiting for something (like input it needs to go on).  A program which depends a lot on I/O will have many short CPU bursts, whereas a program which is CPU-bound will have a few long CPU bursts.  CPU bursts vary hugely but tend to last around 1 or 2 milliseconds.  <span class="shh">Computers, you are amazing.</span><br /><br /><br /><span class="sig">Scheduling Schemes</span><br />The <strong>short-term scheduler</strong> (or CPU scheduler) decides, when the CPU becomes idle, what process to run from a list of processes which are ready to execute, and gives that process CPU.  The list of &#8220;ready&#8221; processes can be a queue, or a priority queue, or a tree, or a linked list, etc.<br /><br />Here&#8217;s a picture of the states of a process&#8217;s thread:<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/_SdPKamJbrgg/TTZSV_VgYpI/AAAAAAAAAEs/lJ40rGVHVbo/s1600/states%2Bof%2Ba%2Bthread.jpg" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="285" width="320" src="http://4.bp.blogspot.com/_SdPKamJbrgg/TTZSV_VgYpI/AAAAAAAAAEs/lJ40rGVHVbo/s320/states%2Bof%2Ba%2Bthread.jpg" /></a></div><br /><br />The scheduler has two different types of schemes.<br /><br />In the <strong>nonpreemptive/cooperative</strong> scheduling scheme, once a process gets CPU allocated to it, the process keeps it until it terminates or goings into the waiting state.  It happens when <ul><li>a process goes from running to waiting state (i.e. because of an I/O request or when a child is being terminated)</li><li>a process terminates</li></ul><br /><strong>Preemptive</strong> scheduling is the opposite; if a process has CPU allocated to it, it still might be interrupted and have its resources allocated to another process.  Preemptive scheduling or nonpreemptive scheduling happens when<ul><li>a process switches from waiting to ready state (i.e. because completion of I/O)</li><li>a process goes from running state to ready state (i.e. because of an interrupt)</li></ul><br />In preemptive scheduling, since data is shared, you can run into concurrency problems where one of the processes sharing data alters it while others are reading it, giving everyone else different information then you may have intended them to have.<br /><br />Since interrupts can happen at any time, and since they can&#8217;t always be ignored by the kernel, code that can be affected by interrupts by be guarded from being used by something else, probably using mutexes, though the book doesn&#8217;t say that right now.<br /><br /><br /><span class="sig">Dispatcher</span><br />The dispatcher is what grants a process control to the CPU, once its been scheduled to have it.  The dispatcher takes care of switching context, switching to user mode, and jumping to the proper space in the user program to execute.<br /><br /><br /><span class="sig">Scheduling Criteria</span><br />Scheduling performance looks to do these things:<ul><li>CPU utilization: keep CPU has busy as possible</li><li>Throughput: make sure that the number of processes that are completed per unit of time is high (i.e. optimize so that 6 processes are completed per second)</li><li>Turnaround time: make sure that processes get completed fast</li><li>Waiting time: make sure that processes do not wait in the ready queue for a long time (and how long it takes a process to execute doesn&#8217;t matter)</li><li>Response time: make sure that processes&#8217; first output is fast (the ones following do as well; but response time refers to the time it takes for a process to give its first output)</li><li>Fairness: make sure that all the processes get a chance at using the resources</li><li>No starvation: make sure that no process never get a chance to run</li><li>Deadlines: make sure jobs finish by the time they need to</li><li>Thread locality: threads that are part of the same process run faster when they&#8217;re on the same processor (because they share memory), so it&#8217;s best to make sure this happens</li><li>etc: etc etc</li></ul><br />Obviously one algorithm can&#8217;t do all of these things though, so you&#8217;ll have to decide which algorithm is best for which kind of system.<br /><br /><br /><span class="sig">Scheduling Algorithms</span><br />All scheduling is concerned with is which of the processes in the ready queue should be allocated to the CPU.<br /><br />In <strong>first-come, first-served</strong> scheduling, the process that requests the CPU first gets the CPU first.  The ready queue in this case can be implemented as a FIFO queue.  The code is simple, but it&#8217;s slow.  One example of how it can be inefficient:<ol><li>In a ready queue, there&#8217;s one CPU-bound process, and many I/O-bound processes.</li><li>The CPU-bound process gets the CPU.  The I/O-bound processes wait.</li><li>The CPU-bound finishes, goes through its I/O burst.  With the CPU freed up, the I/O-bound processes use up the CPU for their CPU bursts &#8211; all quickly, since the CPU burst for an I/O-bound process is short &#8211; and then go back to the queue.</li><li>CPU is now idle, and eventually moved to the ready queue.  Once it gets the CPU, the other I/O-bound processes are left waiting again.</li></ol><br />This situation, where a lot of processes are waiting for one big processes to finish, is a <strong>convoy effect</strong>.  It means that the available CPU is lowered, and device utilization is poor.  Also, since a process that gets the CPU keeps it until the end, first-come first-serve scheduling is nonpreemptive.<br /><br /><br />In <strong>shortest-job-first scheduling</strong> (SJF), each process is associated with the length of its next CPU burst, and the process that gets chosen to have the CPU next is the one which has the smallest CPU burst.  This algorithm gives the minimum average waiting time for a given set of processes.  Disadvantage: it&#8217;s hard to know the length of the next CPU request.  Therefore, it&#8217;s best for long-term scheduling, when you can predict the length of a CPU burst since it will likely be close to the length of the CPU bursts before it.  The SJF algorithm is <em>optimal</em>, and can be either preemptive or nonpreemptive, where the difference is that the scheduler may allow interruptions to a currently running process.<br /><br /><br />The <strong>priority scheduling algorithm</strong> is a more general case of the SJF algorithm.  In this algorithm, each process is associated with a certain priority, and the processes with the greatest priority are chosen to run next.  Processes with equal priority are executed first-come, first-serve.  Priorities can be defined internally, in which case priority would be based on things like time limits or memory requirements.  Externally-defined processes would be set by criteria like whether the process is more important and stuff.<br /><br />Disadvantage: <strong>indefinite blocking</strong> or <strong>starvation</strong>, which can happen if some low-priority processes are kept waiting forever because there&#8217;s always someone more important to be resource-fed.  One solution: <strong>aging</strong>, which would increase the priority of a process the longer it&#8217;s been waiting.<br /><br /><br /><strong>Round-robin scheduling</strong> is especially for timesharing systems, and it&#8217;s like first-come first-serve scheduling but with preemption so that the system can switch between processes.  The ready queue is FIFO and circular, and the scheduler allocates CPU to each process, up to one unit of time, or a &#8220;time quantum.&#8221; Processes are chosen from the top of the ready queue and dispatched.  If the processes has a CPU burst of less than 1 time quantum, then it is allowed to continue; if not, then an interrupt will go off, and after having run for a quantum, that process will be placed at the tail of the ready queue.  This means: fairness, because all processes get a turn at the CPU, and also no starvation, because nothing will get ignored.<br /><br />Disadvantage: average waiting time if you&#8217;re going by RR scheduling is long, and the performance depends a lot on the size of the time quantum.  If it&#8217;s too large, than RR is the same as first-come first-serve; but if it&#8217;s too small, than it becomes like &#8220;processor sharing&#8221; and rather than each of n processes getting the whole CPU for a little bit, each process will get 1/nth of CPU.<br /><br /><br />In <strong>multilevel queue scheduling</strong>, processes are classified into different groups, i.e. <strong>foreground</strong> for interactive processes and <strong>background</strong> for batch processes, which have different scheduling needs.  In a multilevel queue schedule algorithm, the ready queue is split up into several separate queues and processes are assigned permanently to one queue based on some characteristic like process type or priority.  Each queue would have its own scheduling algorithm, and each queue would have its own priority, so that a lower-priority queue would be unable to execute anything until higher-priority queues are empty.<br /><br /><br /><br /><span class="sig">Thread Scheduling</span><br />Since we&#8217;re talking about the OS here, we&#8217;re talking about scheduling kernel-level threads, not user-level threads.  User-level threads are scheduled to run in a scheme using <strong>process-contention scope</strong>, where threads are chosen to run based on priority.  Kernel-threads are scheduled with <strong>system-contention scope</strong>.<br /><br /><br /><span class="sig">Other notes because I&#8217;m too lazy to finish this chapter in its entirety</span><br />If a system has multiple processors, usually each processor is allowed to schedule itself independently, with its own private queue of processes or threads.<br /><br /><br />Sources:<br /><em>Operating System Concepts (8th Edition)</em>, Silberschatz, Galvin and Gagne, ISBN 978-0-470-12872-5.<br />Class lecture 4 notes<br />Class section 3 notes</div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/compsci/'>compsci</a>, <a class='category' href='/blog/categories/notes/'>notes</a>, <a class='category' href='/blog/categories/operating-systems/'>operating systems</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/18/last-night-andou-asked-me-if-you/">Last Night Andou Asked Me, if You Started Writing a Book Right Now, What Would It Be About?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-18T00:00:00-08:00" pubdate data-updated="true">Jan 18<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<span class="sig">That morning at the last stop, when Sara unzipped her lunchbox to trade her mother&#8217;s lasagna (secretly full of vegetables) for a bag of chips, a man walked onto the school bus, shot the bus driver, kicked his body out the door, and took the steering wheel.</span></div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/drabble/'>drabble</a>, <a class='category' href='/blog/categories/writing/'>writing</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/16/q-standard-annotation-language-sal/">[Q&a] Standard Annotation Language (SAL)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-16T00:00:00-08:00" pubdate data-updated="true">Jan 16<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<span class="sig">What does <code>__inout</code>, etc. mean?</span><br />For OS I&#8217;m plumbing the depths of a Windows kernel and keep running into these things in the code that look like <code>__in</code>, or <code>__out</code>, or even <code>__inout</code>&#8230;for example:<br /><br /><pre class="brush:xml">NTKERNELAPI<br />VOID<br />FASTCALL<br />ExAcquireFastMutexUnsafe (<br />    __inout PFAST_MUTEX FastMutex<br />    );<br /></pre><br />I initially found <a href="http://nuubu.blogspot.com/2011/01/q-operating-systems-etc.html">something</a> about the usage of <code>_</code> and <code>__</code> in code, and it turns out that <code>__</code> is just an annotation which will help programmers know more about the intended usage of the code, like comments, and are ignored by the compiler.  <code>__inout</code> stuff is no different.<br /><br />These annotations are part of Microsoft&#8217;s SAL, or, Standard Annotation Language, and define the proper use of buffers, which are regions of data that have been allocated and are represented as pointers.  If a pointer is pointing to a buffer, there&#8217;s no good way to know how big a buffer is in compile time; with usage of SAL, you can make explicit exactly how big a buffer is.  It also helps to show how a function uses its parameters &#8211; what it thinks that the parameter is, and what it will do with that parameter when it finishes.<br /><br />This is relevant because in C, a function can take a value or a pointer as an argument.  Arguments can be used as input (i.e. taking values to add together), or as output (i.e. taking a pointer to something which will store your output value.) But, if you were to leave out these annotations, you wouldn&#8217;t know if an argument a function takes is supposed to be used as function input or output.<br /><br /><code>__in</code>, <code>__out</code>, and <code>__inout</code> are all examples of &#8220;Usage&#8221; annotations.<br /><br />So, if you see something like<br /><br /><pre class="brush: xml">void * memset(<br /> __out_bcount(s) char *p,<br /> __in int v, <br /> __in size_t s);<br /></pre><br />This means that the function <code>memset</code> expects variables <code>int v</code> and <code>size_t s</code> to be valid when the function is called (also called &#8220;valid on input&#8221;).  Also, the buffer <code>char *p</code> will be initialized by this function, will be written to by this function, and will be valid when the function returns.<br /><br />In simpler terms &#8211; <code>v</code> and <code>s</code> are definitely meant to be <code>memset</code>&#8217;s input, and <code>p</code> is meant to be the output.<br /><br /><br />Sources:<br /><a href="http://207.46.16.248/en-us/library/ff550230%28VS.85%29.aspx">MSDN: Overview of Annotations for Drivers</a><br /><a href="http://msdn.microsoft.com/en-us/library/ms235402%28v=vs.80%29.aspx">MSDN: SAL Annotations</a><br /><a href="http://blogs.msdn.com/b/michael_howard/archive/2006/05/19/602077.aspx">MSDN: A Brief Introduction to the Standard Annotation Language</a></div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/c/'>c</a>, <a class='category' href='/blog/categories/operating-systems/'>operating systems</a>, <a class='category' href='/blog/categories/programming/'>programming</a>, <a class='category' href='/blog/categories/qna/'>qna</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/14/q-semaphore-mutex/">[Q&a] Semaphore, Mutex</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-14T00:00:00-08:00" pubdate data-updated="true">Jan 14<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<span class="sig">What&#8217;s a semaphore?</span><br />A semaphore is a procted variable or abstract data type.  It relates to controlling access to some resource shared by processes running simultaneously.  They&#8217;re useful in preventing race conditions and deadlocks (though they don&#8217;t prevent them completely).<br /><br />Semaphores don&#8217;t keep track of which resources are free or who is using the resources; they only keep track of whether resources are free.  There are two types of semaphores: <b>counting semaphores</b> and <b>binary semaphores</b>.  Counting semaphores only keep track of how much of the resource is available at a given time (i.e. &#8220;all of my resources are being used up now&#8230;oh, someone came back, okay, now I have one free which you can use&#8221;).  Binary semaphores only keep track of whether a resource is being used or not, i.e. whether it is presently unavailable or available.<br /><br />Semaphores can&#8217;t prevent errors that occur if a process that has acquired a lock for the resource forgets to release the lock, etc.<br /><br />Also, errors can still happen if there are different resources managed by different semaphores and processes need to use more than one resource at a time.  An interesting thing: the <a href="http://en.wikipedia.org/wiki/Dining_philosophers_problem">dining philosophers problem</a>, which illustrates the problems with multiple processes sharing multiple resources.  Essentially, five philosophers are sitting at a table, either eating or thinking.  They each have a fork to their left and right, and there is a bowl of spaghetti in the middle of the table.  The philosophers need to serve themselves and eat spaghetti with both forks, but don&#8217;t speak to each other, so one may pick up a left fork and wait for his right fork to become free so that he may eat.  Meanwhile, the guy using his right fork may be waiting for his own other fork to become free.  And so on, and so forth.<br /><br />A semaphore in this case would be like a person who is aware of which forks are being used and which ones are available, so that when the philosophers try to eat, they have to consult him first, and he&#8217;ll tell them whether there are any forks to pick up which they can use.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/_SdPKamJbrgg/TS9WBJzlB7I/AAAAAAAAAEk/QEHATD_NLWw/s1600/proceed%2Bcat.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="283" src="http://3.bp.blogspot.com/_SdPKamJbrgg/TS9WBJzlB7I/AAAAAAAAAEk/QEHATD_NLWw/s320/proceed%2Bcat.jpg" width="300" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">semaphore cat is semaphore</td></tr></tbody></table><br />One problem with semaphores: <strong>busy waiting</strong>, which is what you need to put into processes that are waiting for other processes to finish executing their critical section.  (Or, what philosophers are doing when they are waiting for their neighbors to finish eating.) The busy waiting is basically just a loop waiting for the mutex to free up, and it wastes CPU that another process could be productive with.  A process that does this is a <strong>spinlock</strong>, because &#8220;the process &#8216;spins&#8217; while waiting for the lock.&#8221;  Essentially, if a program knows it needs a lock before it does the next bit of code, it sits in a while loop until the needed lock is freed up.<br /><br /><br /><span class="sig">What&#8217;s a mutex?</span><br />A mutex is essentially a binary semaphore, though mutex specifically describes an abstraction which prevents two processes from executing the same piece of code or accessing the same data at the same time.  Basically, it limits access to a single resource, to one &#8220;owner&#8221; who is at that time allowed to do whatever it wants with it before releasing the mutex so someone else can do something.<br /><br /><br /><span class="sig">What does IRQL mean?</span><br />IRQL stands for Interrupt Request Level, and has something to do with a process, and, given incoming interrupts, which one of those interrupts should be able to interrupt the running process, and which should not.  For instance, if you&#8217;re just quickly trying to modify a single variable, then you don&#8217;t want anything to interrupt you.<br /><br />Basically (I think), each thread has its own IRQL.  If the OS receives another thread to run which has a higher IRQL than that first thread, however, then it will pause the lower-IRQL thread to run the higher-IRQL code.  Weird deadlocks may occur in this case, however, if:<br /><ol><li>lower-IRQL thread is running, and acquires a lock</li><li>lower-IRQL thread is interrupted; higher-IRQL thread begins execution</li><li>higher-IRQL thread needs the lock that lower-IRQL thread has</li><li>higher-IRQL lock cannot continue without that lock</li><li>lower-IRQL thread cannot continue without higher-IRQL finishing execution</li></ol>So, what you would want to do in this case is make sure that a thread that obtains a lock cannot be interrupted.<br /><br />If it does so happen that you don&#8217;t want any interrupts to your thread, then make sure that the IRQL is raised to <code>APC_LEVEL</code>.  This level is pretty much only run when you&#8217;re using Fast Mutexes.  Part of the reason why Fast Mutexes are faster than normal Mutexes in the Windows kernel is because they deny APC interrupts.  APC interrupts originate from the processor, toward itself or another processor.<br /><br /><br /><span class="sig">How do I use a mutex in the Windows NT kernel?</span><br /><a href="http://www-user.tu-chemnitz.de/~heha/oney_wdm/ch04f.htm">Here&#8217;s</a> a start&#8230;<br /><br /><br />Sources:<br /><a href="http://en.wikipedia.org/wiki/Semaphore_%28programming%29">Wikipedia: Semaphore (programming)</a><br /><a href="http://us.generation-nt.com/answer/fast-mutexes-guarded-mutexes-help-27627542.html">GNT: Fast mutexes and guarded mutexes?</a><br /><a href="http://blogs.technet.com/b/askperf/archive/2009/07/21/the-basics-of-mutexes-and-spin-locks.aspx">TechNet: The Basics of Mutexes and Spin Locks</a><br /><a href="http://blogs.msdn.com/b/doronh/archive/2010/02/02/what-is-irql.aspx">MSDN: What is IRQL?</a><br /><a href="http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBMQFjAA&amp;url=http%3A%2F%2Fdownload.microsoft.com%2Fdownload%2Fe%2Fb%2Fa%2Feba1050f-a31d-436b-9281-92cdfeae4b45%2FLocks.doc&amp;rct=j&amp;q=ex%20initialize%20fast%20mutex%20example&amp;ei=yJEzTf7ILIGqsAP4rqiFBg&amp;usg=AFQjCNEWYt0oNPRzKd8sXHFsm_CgrHKZBw&amp;sig2=4FQ-QHdGlj_42VPdrKqIxA&amp;cad=rja">Windows Hardware and Driver Central: Locks, Deadlocks, and Synchronization</a><br /><em>Operating System Concepts (8th Edition)</em>, Silberschatz, Galvin and Gagne, ISBN 978-0-470-12872-5.</div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/compsci/'>compsci</a>, <a class='category' href='/blog/categories/notes/'>notes</a>, <a class='category' href='/blog/categories/qna/'>qna</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/14/operating-systems-notes-chapter-2-3/">Operating Systems Notes [Chapter 2, 3]</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-14T00:00:00-08:00" pubdate data-updated="true">Jan 14<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<span class="sig">OS Structures</span><br /><strong>Command shells</strong>, I guess, make it so that users can actually speak directly to the OS and do something, rather than writing a program to speak with the OS.  I didn&#8217;t realize this.<br /><br />To make requests of the OS otherwise, you&#8217;d need to do a <strong>system call</strong>.  Types of requests that you might make the the OS would involve process control, and file and device manipulation (i.e. writing, reading from a mouse or file).<br /><br />Since operating systems are huge and all its parts tend to be interconnected, some better ways to program them involve using <strong>layers</strong> or <strong>microkernels</strong>.<br /><br />In the layered system, the OS structure is broken into layers, with the lowest being hardware and the highest being the user interface.  Layers can speak to adjacent layers, but not further.  The advantage is that you could be working on one layer at a time, and assume that if it works that you can move on to the next layer.  The issue with this is that you make the assumption that layers would not need to have communication across layers, when really sometimes layers <em>do</em> need to speak across other layers in real life, i.e. a user program needing to alter a file.<br /><br />In a microkernel system, all non-essential kernel programs are implemented as system and user-level programs, which means that the kernel is as small as it can possibly be to code it.  In general, this minimal kernel would provide only process and memory management along with a way to communicate.  The main function of the microkernel now is only to facilitate communication between the client program and other services, with messages.  Advantage is that it&#8217;s also easy to extend a microkernel&#8217;s functionality.  Disadvantage: poor performance if you need to make tons of system calls.<br /><br />In general, modularity is important.<br /><br /><br /><span class="sig">Processes</span><br />A process is a program in execution.  It consists of an address space, at least one thread, and a set of OS resources.  The address space is memory which contains code and data for the running program, as well as data relevant to the threads: registers, instruction pointer, stack and stack pointer.  A process also has a set of OS resources, such as open files, network connections, etc.  A process contains everything you need to run the program, or to restart it, if it gets interrupted.<br /><br />Processes may be independent or cooperating, depending on how they interact with each other.  <strong>Cooperating processes</strong> require some way to communicate with each other: either through shared memory (through shared variables), or message passing (through a pipe).  With shared memory, application programmers manage the communication, whereas with message-passing, the operating system has to take care of communication.  Shared memory and message passing aren&#8217;t mutually exclusive.<br /><br />You can also communicate across computers in a client-server system using sockets, remote procedure calls, or pipes.  A socket is an endpoint for communication, so a connection between two applications consists of two sockets, one for each.  <a href="http://nuubu.blogspot.com/2011/01/remote-procedure-calls-rpc.html">Remote procedure calls</a> are when an application calls another application&#8217;s function across a system.  Pipes come in two flavors: ordinary, which allows parent and child processes to communicate; and named, which allows two unrelated processes to speak with each other.</div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/compsci/'>compsci</a>, <a class='category' href='/blog/categories/notes/'>notes</a>, <a class='category' href='/blog/categories/operating-systems/'>operating systems</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/14/flexas3-errors-with-stage-and-event/">Flex/AS3: Errors With Stage and Event Listeners</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-14T00:00:00-08:00" pubdate data-updated="true">Jan 14<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
This is a continuation of myself attempting to find out how to <a href="http://nuubu.blogspot.com/2010/12/problem-how-do-i-use-flex-components-to.html">use Flex UI components with a previously created AS3 project</a>.<br /><br />And it works fine, except for this error which I get when running in debug mode:<br /><br /><code>TypeError: Error #1009: Cannot access a property or method of a null object reference.</code><br /><br />It turns out that this error refers to a part of the AS3 code which does something with this variable called the stage.  This stage variable was an instance of the <strong>Stage</strong> class, and it&#8217;s made when the Flash Player starts.  SWF files are loaded into the stage container, and all of their objects that display are DisplayObjects, and are children of the stage.  All DisplayObject instances also have a <code>stage</code> property which references the stage that is displaying them.  <span class="shh">Also: Stage is a singleton class, meaning there&#8217;s only one of them, and it can&#8217;t be modified by lowly programmers, and is always at the top level of the display hierarchy within a Flash Player.</span><br /><br />Anyway, the error occurs (I think) because my parent (UI) application loads onto the stage before the embedded (AS3) application.  The AS3 app then tries to use the <code>stage</code> property before it&#8217;s even on the stage.  The solution is to make sure that the embedded SWF is on the stage before you try instantiating or doing anything.<br /><br />To fix it, I essentially wrapped up offending code in an event listener which would wait until the stage was loaded to execute.  So, this code which lets the stage listen for keyboard input:<br /><br /><pre class="brush: cpp"><br />stage.addEventListener(KeyboardEvent.KEY_DOWN, keyDownHandler);<br /><br /></pre><br />became<br /><br /><pre class="brush: cpp"><br />addEventListener(Event.ADDED_TO_STAGE, function (e:Event):void {<br />  stage.addEventListener(KeyboardEvent.KEY_DOWN, keyDownHandler);<br />});<br /><br /></pre><br />Not sure if this is good style or not.<br /><br /><br />Sources:<br /><a href="http://www.kirupa.com/forum/showthread.php?p=2129548#post2129548">Kirupa: ActionScript 3 Tip of the Day</a><br /><a href="http://jaycsantos.com/flash/do-you-know-actionscript-as3-stage/">jaycsantos.com: Do you know ActionScript?  AS3 Stage</a><br /><a href="http://board.flashkit.com/board/showthread.php?t=792605">Flash Kit: TypeError: Error #1009</a></div>
<h2>Comments</h2>
<div class='comments'>
<div class='comment'>
<div class='author'>Lewis R Strasburg</div>
<div class='content'>
Nice solution, it worked great for me.  I only needed to reference the stage at the very beginning for a &quot;Press Any Key to Start&quot; scenario, and it wasn&#39;t working until now.  Thanks!</div>
</div>
</div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/actionscript/'>actionscript</a>, <a class='category' href='/blog/categories/flash/'>flash</a>, <a class='category' href='/blog/categories/flex/'>flex</a>, <a class='category' href='/blog/categories/troubleshooting/'>troubleshooting</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/09/c-typedef-struct/">C: Typedef, Struct, Extern</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-09T00:00:00-08:00" pubdate data-updated="true">Jan 9<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<span class="sig">What is <code>typedef</code> and <code>struct</code>?</span><br />A struct in C is a structured type that combines a set of labeled objects of different types, into a single object.  So, kind of like a Java object with its own variables.<br /><br />An example of a struct you could make would be<br /><br /><pre class="brush: cpp">struct pokemon {<br />   char* nickname;<br />   char* species;<br />   int level;<br />   char* item;<br />};<br /></pre><br />Then, to create a new Pokemon struct, I could say<br /><br /><pre class="brush: cpp">struct Pokemon pikachu;<br /></pre><br />And to access the new struct&#8217;s nickname, I could say something like<br /><br /><pre class="brush: cpp">pikachu.nickname;<br /></pre><br />The keyword <code>typedef</code> is just meant so that you can assign different &#8220;names&#8221; to existing types, in case you may get confused.  For instance, you might have code that looks like<br /><br /><pre class="brush: cpp">char* pokemon_name;<br />char* item;<br /><br />void buy(char* item) {<br />   // remove money from the user<br />}<br /></pre><br />Though <code>buy</code> can take any char*, what you really want it to do is only accept char* that represent items, not Pokemon nicknames.  So, you can use typedef to distinguish that pokemon_name and item are completely unrelated variables which happen to both be of type char*.<br /><br /><pre class="brush: cpp">typedef char* pokemon_name;<br />typedef char* item;<br /><br />pokemon_name pokemon_nickname;<br />item item_name;<br /><br />void buy(char* item) {<br />   // remove money from the user<br />}<br /></pre><br />This distinction is only for the programmer; the C/C++ compiler considers both things to be char*, and won&#8217;t give any errors if you happen to try and purchase a Pokemon.  <span class="shh">You cheat.</span><br /><br /><code>typedef</code> can also make declarations easier; for instance, if I did something like<br /><br /><pre class="brush:cpp">typedef struct Pokemon {<br />   char* nickname;<br />   char* species;<br />   int level;<br />   char* item;<br />} Pokemon;<br /></pre><br />Then, instead of having to write <code>struct Pokemon pikachu;</code> like earlier, I could make a new Pokemon just by saying<br /><br /><pre class="brush: cpp">Pokemon pikachu;<br /></pre><br /><br /><span class="sig">What does <code>extern</code> mean?</span><br />The <code>extern</code> declaration indicates the existence of, and type of, a global variable or function.  <code>extern</code> means that something is defined externally to the current module (so if your current .c file is using it, that means that another, included file actually defined it?).  <code>extern</code> can be left off because the linker collapses the multiple definitions into a single one, but to use the <code>extern</code> keyword is cleaner because it defines the global variable in one place, and everything else makes <code>extern</code> references to it.  <br /><br />If a program has a variable that&#8217;s declared as <code>extern</code>, the program won&#8217;t reserve any memory for the variable in the scope that it was declared.  Which is why you need to use the <code>extern</code> keyword in programs that are using the declaration, but haven&#8217;t defined it.<br /><br /><br /><br />Source:<br /><a href="http://en.wikipedia.org/wiki/Struct_%28C_programming_language%29">Wikipedia: Struct (C programming language)</a><br /><a href="http://wiki.answers.com/Q/What_is_the_use_of_extern_in_C">Answers.com: What is the use of extern in C</a></div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/c/'>c</a>, <a class='category' href='/blog/categories/notes/'>notes</a>, <a class='category' href='/blog/categories/programming/'>programming</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/06/q-operating-systems-etc/">[Q&a] Operating Systems, C, Etc.</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-06T00:00:00-08:00" pubdate data-updated="true">Jan 6<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<b>How do you switch drives with Windows command line?</b><br />If you want to go to drive <code>Z:</code>, rather than <code>cd Z:</code>, you can just type <code>Z:</code>.<br /><br /><br /><b>What do you do when you demux something?</b><br />A demultiplexer is a device that takes a single input signal and routes it to one of many output lines.  It&#8217;s often used with a multiplexer on the sending end.  So, when you demux a signal, you accept it, and then based on some parameter send it to a different function or something. <span class="shh">I think?</span><br /><br /><br /><b>What&#8217;s a virtual machine?</b><br />&#8220;A software implementation of a programmable machine&#8221; &#8211; so, a simulation of a computer, on a computer.<br /><br /><br /><b>What&#8217;s the difference between a function in C and a method in Java?</b><br />All C/C++ programs require a function named <code>main</code> and besides that can have numerous other functions.  In Java, functions can&#8217;t stand by themselves &#8211; they have to be part of a class.  Functions that are part of a class are usually called methods.<br /><br />Also, it seems that methods are called by reference values, which are objects, perhaps like <code>Object.method()</code>, as opposed to just <code>value = function()</code>?  Also, &#8220;method&#8221; is used more often in Java, most likely because Java has objects and C/C++ does not.<br /><br /><br /><b>What&#8217;s Windows NT?</b><br />It&#8217;s a family of operating systems created by Microsoft, which were meant to complement consumer versions of Windows that were based on MS-DOS.  NT was the first fully 32-bit version of Windows.  Windows 2000, Windows XP, Windows Server 2003, Windows Vista, Windows 7, etc., are based on Windows NT, though they aren&#8217;t branded the same way.  NT was once expanded to mean &#8220;New Technology&#8221; and stood originally for &#8220;N-Ten,&#8221; the codename of the Intell i860 XR processor that the OS was originally developed for, but NT has no particular meaning anymore.  Sidenote: it is really confusing that there&#8217;s an operating system called Windows <i>Server</i> 2003.  <i>Wat.</i>  I guess this calls into question what exactly a server is supposed to be.<br /><br /><br /><b>What is a server?</b><br />A server can refer to:<br /><ul><li>a computer program running as <a href="http://nuubu.blogspot.com/2011/01/intro-to-distributed-systems.html">a service</a>, to serve the needs or requests of other programs</li><li>a physical computer running one or more such services, to serve the needs of programs running on other computers in the same network</li><li>a software/hardware system such as a database server, file server, mail server, print server</li></ul>I guess server operating systems are operating systems that have certain features which make them good servers &#8211; such as limited GUI, flexible/advanced networking capability, tight security, backup securities, transparent data transfer between different volumes or devices &#8211; and maybe that&#8217;s what the Windows Server operating systems are all about.<br /><br /><br /><b>How do you press Ctrl+Alt+Delete in a virtual machine?</b><br />In a virtual machine in MS Virtual PC 2007, rather than pressing the keys, go to the <code>Action</code> tab and select <code>Ctrl+Alt+Delete</code>.<br /><br /><br /><b>What does the <code>_</code> or <code>__</code> mean in C programs?</b><br />There&#8217;s no particular meaning.  <code>_</code> is meant to identify system variables/functions, <code>__</code> is meant to identify metadata.  Additionally, when you include headers in C, you <br />add in a lot of code and functions and variables; to prevent these imported names from causing duplicates with the variables/names of your own stuff, it&#8217;s best to avoid using variables/functions which start with underscores in your own program.<br /><br /><br /><br /><br />Source:<br />Jimmy<br />Wikipedia: <a href="http://en.wikipedia.org/wiki/Multiplexer">1</a>, <a href="http://en.wikipedia.org/wiki/Virtual_machine">2</a>, <a href="http://en.wikipedia.org/wiki/Windows_NT">3</a>, <a href="http://en.wikipedia.org/wiki/Server_%28computing%29">4</a><br /><a href="http://www.geekinterview.com/question_details/31163">GeekInterview: What is the difference between methods and functions?</a><br /><a href="http://www.dickbaldwin.com/java/Java008.htm">Dick Baldwin: Similarities and Differences between Java and C++</a><br /><a href="http://bytes.com/topic/c/answers/540408-_-__-significance">Bytes: _ and __ significance</a></div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/c/'>c</a>, <a class='category' href='/blog/categories/compsci/'>compsci</a>, <a class='category' href='/blog/categories/notes/'>notes</a>, <a class='category' href='/blog/categories/qna/'>qna</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/06/operating-systems-notes-lecture-2/">Operating Systems Notes [Lecture 2]</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-06T00:00:00-08:00" pubdate data-updated="true">Jan 6<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
An operating system provides <a href="http://nuubu.blogspot.com/2011/01/operating-system-notes-chapter-1.html">a middle ground</a> between hardware and software, so making an operating system depends a lot on what hardware you have underlying it.  Computer hardware, however, has the amazing capacity to (at least until fairly recently) follow <strong>Moore&#8217;s Law</strong>, which states that <em>the number of transistors that can be placed inexpensively on an integrated circuit has doubled approximately every two years.</em>  I guess this trend has persisted for half a century, making it so that processors and memory have become ridiculously cheap, ridiculously fast.<br /><br />What this means is, the hardware conditions that you assume and initially create your operating system for may not hold true, and then you&#8217;ll be screwed.<br /><br />One interesting thing is that, though processor performance has increased throughout the years, disk capacity has become 10x as fast as processor performance.  Disk performance has <em>not</em> kept up, having only increased by a factor of 200 since 1983 (from 500 kb/s to ~100 mb/s).  Bandwidth presently is 100x as fast as processor performance, and 10x as fast as disk performance.<br /><br />An obvious implication of these unexpected growths is this: What happens if you have always designed systems so that they spend processing power so as to save &#8220;scarce&#8221; storage and bandwidth?<br /><br />Another thing: operating systems must load programs into main memory in order to execute them, but main memory is not large enough to hold everything all at once, and occasionally you&#8217;ll have to fetch data from the disc.  A computer going out of its way to fetch data from the disc is like a human going out of its way to fetch something from Pluto.  <em>Uphill.  Both ways.  In space.</em><br /><br />In general, hardware can dictate a lot about how simple or complex it will be to implement an OS.  Early OS (like DOS) didn&#8217;t have virtual memory, because the hardware didn&#8217;t allow it; and until recently, Intel-based PCs still didn&#8217;t support 64-bit addressing, though other platforms have had it since forever, like IBM, MIPS, etc.<br /><br />Features were also sometimes built into hardware to support OS building, like timer operation, memory protection, interrupts and exceptions, system calls.<br /><br /><strong>System calls</strong> are there so that the user can call an OS procedure, which is otherwise protected and kept out of hand&#8217;s reach on top of the refridgerator in kernel mode, so that the user doesn&#8217;t eat it all and spill it all over everything.  Once the user is done having a dessert cookie, the OS moves back into user mode, and relinquishes control back to the user.<br /><br />The kernel must save the state that the user was in before the user wanted to eat a cookie, so that it can remember what it was doing before it was interrupted, and resume.<br /><br />So in general the OS sits around waiting for events to happen that it can deal with.  Events can be interrupts, which are sent by programs, or they can be exceptions.  <strong>Exceptions</strong> are thrown by hardware once it detects certain rules being violated (like if someone tries to write to a read-only file), and when they happen control must be passed to the OS&#8217;s handler, with the saved state at the time of the fault.  Exceptions are a performance optimization in that the OS doesn&#8217;t have to deal with figuring out when things are going wrong &#8211; the hardware detects the violation &#8211; and detecting exceptions would be a big hassle for the OS because those extra checks would have to be written into the kernel, taking valuable space.<br /><br />In the meantime, interrupts allow for <strong>asynchronous I/O</strong>.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/_SdPKamJbrgg/TSasz1jh5fI/AAAAAAAAAD8/9lBMBA1S9E0/s1600/os%2Bio%2Bcontrol.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="320" src="http://1.bp.blogspot.com/_SdPKamJbrgg/TSasz1jh5fI/AAAAAAAAAD8/9lBMBA1S9E0/s320/os%2Bio%2Bcontrol.jpg" width="246" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">this is how we deal with I/O interrupts around here</td></tr></tbody></table><br />So, I/O interrupts can happen at any time, any place.  How do you protect yourself from getting interrupted so much that you end up never getting anything done?  Your OS will have to be able to <strong>synchronize concurrent processes</strong> by guaranteeing that short instructions (read-modify-write) will happen automatically; or maybe, by not letting certain processes get interrupted until their finished; or maybe, by having special atomic instructions, such as the read-modify-write, which you execute immediately.<br /><br /><strong>Concurrent programming</strong> is a huge deal and is a huge difference between systems programming and &#8220;traditional application programming,&#8221; which I&#8217;m starting to feel is like, the lame computer programming.<br /><br /><br /><br />Sources:<br />Lecture 2 notes<br /><a href="http://en.wikipedia.org/wiki/Moore's_law">Wikipedia: Moore&#8217;s Law</a></div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/compsci/'>compsci</a>, <a class='category' href='/blog/categories/notes/'>notes</a>, <a class='category' href='/blog/categories/operating-systems/'>operating systems</a>
  
</span>

</a>


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2011/01/06/operating-system-notes-chapter-1/">Operating Systems Notes [Chapter 1]</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-01-06T00:00:00-08:00" pubdate data-updated="true">Jan 6<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content"><div class='post'>
<span class="sig">What is an operating system?</span><br />An <strong>operating system</strong> is a program that manages computer hardware, as well as providing a basis for application programs.  There are a variety of types of operating systems, not all of them for making it easy for a user to use games or software on a computer &#8211; for instance, you might have a mainframe operating system, which is an OS designed to optimize utilization of hardware.<br /><br />Computers are broken up into 4 &#8220;layers:&#8221;<br /><ul><li><em>Hardware.</em> The central processing unit (CPU), the memory, the input/output devices (like a keyboard, mouse)</li><li><em>Application programs.</em>  The programs you use on your computer &#8211; word processor, PhotoShop, Firefox, etc.</li><li><em>User</em>.  You.</li><li><em>Operating system.</em> Provides an environment for the application programs to do their work, and controls hardware, and coordinates its use amongst the various application programs.</li></ul><br />So, <strong>an operating system makes it so a user can more easily use applications on a computer</strong>, and also <strong>allocates the resources in the hardware</strong> to different applications.  It&#8217;s like the middle layer.<br /><br />The operating system is the one program that&#8217;s running at all times when your computer is on, also called the <strong>kernel</strong>.  (<strong>Systems programs</strong> are associated with OS too, but not the kernel, and <strong>application programs</strong> include all programs that are not associated with the operation of the system.)<br /><br /><br /><span class="sig">CPU and storage</span><br /><strong>CPUs and device controllers</strong> have access to a shared memory, and execute concurrently, competing for memory cycles.  The memory controller makes sure that their access to memory is not messed up, i.e. probably that controllers aren&#8217;t trying to access each other while they&#8217;re being used.<br /><br /><strong>A bootstrap program</strong> is the first program that a computer runs when it is turned on, and is simple, and stored in read-only memory (ROM), which is known as <strong>firmware</strong> and is stored on the computer hardware.  This intializes the system, turning on the CPU and device controllers and memory contents.  The bootstrap program is the one that loads the OS and begins executing it by loading the OS kernel into memory and letting the OS execute its first process, &#8220;init.&#8221;<br /><br /><strong>Interrupts</strong> are sent out by routines and are important because they tell the CPU to stop what it&#8217;s doing and transfer execution to the starting address of the routine.  Interrupts have to be handled quickly, so internet routines are sometimes stored in a table so that they can be called indirectly, but immediately.  This table is usually stored in low memory, and is called an <strong>interrupt vector</strong>.  An interrupt will most likely interfere with another computation that the processor was doing, so the address of that computation must be saved in a system stack, so that the CPU can return and resume later.<br /><br /><strong>Interrupt-driven</strong> refers to how most modern OS sit around and don&#8217;t do anything unless an event happens, i.e. an interrupt.  Code in the OS determine what action to take in the case of each interrupt.  An interrupt service routine is provided that is responsible for dealing with the interrupt.<br /><br /><strong>Low memory</strong> refers to the first hundred or so locations in memory.<br /><br /><strong>Random-access memory (RAM) or main memory</strong> is what stores most of the programs that are run by the CPU.  The CPU can only load instructions from memory.  RAM is implemented in a semiconductor technology called <strong>dynamic random-access memory (DRAM)</strong>.  Though it&#8217;s used to store programs, it&#8217;s usually too small to store all the needed data and programs permanently, and it&#8217;s a volatile storage device in that its contents is lost when computer power is turned off.  This is why secondary storage of memory is usually required.<br /><br /><strong>Secondary storage for memory</strong> must be able to hold large amounts of data permanently.  Example: magnetic disc, cache memory, CD-ROM, etc.<br /><br /><strong>Storage device hierarchy</strong> refers to how the high-level storage devices are fast, but expensive and volatile (like RAM, which is erased when the computer shuts off).  Highest on the pyramid are registers, the cache, and RAM; lower are the hard disk, or offline storage like magnetic tape.<br /><br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/_SdPKamJbrgg/TSZjeXlfXQI/AAAAAAAAAD0/vGkuN6HSEKU/s1600/StorageHierarchy.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="188" src="http://1.bp.blogspot.com/_SdPKamJbrgg/TSZjeXlfXQI/AAAAAAAAAD0/vGkuN6HSEKU/s320/StorageHierarchy.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">(<a href="http://www.ts.avnet.com/uk/products_and_solutions/storage/hierarchy.html">source</a>)</td></tr></tbody></table><br /><br /><strong>Read-only memory (ROM)</strong> can also store programs, but since it can&#8217;t be changed, only static programs are stored here.  ROM is where firmware is stored, and also sometimes (in the past, I guess?) lookup tables for things that don&#8217;t change, like mathematical functions.  Also, used in game cartridges!<br /><br /><strong>Von Neumann architecture</strong> describes a computer design model.  A computer with von Neumann architecture uses a CPU and a single separate storage structure (the memory) to hold both instructions and data.  The computer must also be a stored-program computer, meaning that its data and program instructions are stored in RAM.  &#8220;von Neumann architecture&#8221; and &#8220;stored-program computer&#8221; are generally used interchangeably.<br /><br /><strong>Typical instruction-execution cycle in a system with von Neumann architecture:</strong> An instruction is fetched from memory, and stored in an <strong>instruction register</strong>.  The instruction is then decoded, stored in a register, and the results may be then stored back into memory.  The running program will generate a sequence of memory addresses, which is what the memory unit sees.<br /><br /><br /><span class="sig">I/O structure</span><br /><strong>Device controllers</strong> are part of a general-purpose computer system (the other part being the CPUs), and each one is in charge of a specific type of device.  Depending on the controller, more than one device may be attached; for instance, seven or more devices can be attached to the <strong>small computer-systems interface (SCSI) controller</strong>.  They maintain local buffer storage and a set of special registers.<br /><br /><strong>Device drivers</strong> are kept by the OS, one for each device controller.  The driver provides an interface to allow the OS to interact with the device.  So you might have a device controller for a keyboard, and the driver would allow the OS to obtain the specific key presses.<br /><br /><strong>Direct-access memory (DMA)</strong> is memory that allows bulk data movement, such as input/output to/from a disk.  This makes it so that a block of data can be transferred by the device controller from its own buffer storage into the destination memory, without any help from the CPU, and it emits only one interrupt, when the operation has completed, rather than one interrupt per transferred byte.<br /><br /><br /><span class="sig">Computer-System architecture</span><br /><strong>Single-processor systems</strong> have only one general-purpose CPU capable of executing the instruction set, including instructions from user processes.  In contrast, almost all systems have separate, special-purpose (micro?) processors that are device-specific, such as a processor for the graphics controllers.  Special-purpose microprocessors don&#8217;t run user processes, don&#8217;t usually communicate with the OS (so that it can worry about its own things and go faster), and the presence of special-purpose microprocessors doesn&#8217;t turn a single-processor system into a multiprocessor system.<br /><br /><strong>Multiprocessor systems</strong> are the most common, and also called <strong>parallel systems</strong>.  These systems have two or more processors in close communication, sharing the computer bus and sometimes the clock, memory, and peripheral devices.  They have increased throughput (more processors = more work done in less time), economy of scale (more processors share peripherals, mass storage, and power supplies), and increased reliability (if work is shared between two processors, the failure of one processor will not halt the system, only slow it down).<br /><br /><strong>Graceful degradation</strong> is when a computer system can continue to provide service proportional to the level of surviving hardware.<br /><br /><strong>Fault tolerance</strong> is cooler than graceful degradation, and means that a computer system can suffer a failure to any single component and still continue to operate.<br /><br /><strong>Asymetric multiprocessing</strong> is when each processor in a multiprocessor system is assigned a specific task.  A master processor controls the system, and the other processors look to the master for instruction or have pre-defined tasks.  This is a master-slave relationship.<br /><br /><strong>Symmetric multiprocessing (SMP)</strong> is when each processor performs all tasks within the operating system, and all processors are peers to each other.  Each processor has its own set of registers, and its own local cache, though all processors share physical memory.<br /><br /><strong>Cores</strong> are essentially multiprocessor chips, which are placed on a single chip.  They can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication, and one chip with multiple cores will use tons less power than multiple single-core chips.  Multicore systems are good for server systems, like database and Web servers.  An example dual-core design will have a chip with two cores, each core having its own set of registers and a cache.<br /><br /><strong>Clustered systems</strong> are the same as <a href="http://nuubu.blogspot.com/2011/01/intro-to-distributed-systems.html">distributed systems</a>, I think?!  Oh, no &#8211; I guess clustered systems are made of two or more computer systems (nodes) closely linked through something like LAN, whereas a distributed system is a collection of physically separate computer systems connected by a network to &#8220;provide users with access to the various resources that the system maintains.&#8221;  More about shared resources than parallelization, I guess?  According to this book?<br /><br /><br /><span class="sig">Operating-System Structure</span><br /><strong>Multiprogramming</strong> is when different jobs (code and data) on a computer are organized so that the CPU is always executing something.  A single program can&#8217;t usually keep either the CPU or I/O devices busy at all times.  So, several jobs are kept in memory simultaneously, and whenever one program starts to wait for something (i.e. keyboard input, which it would need to continue), the CPU turns its attention elsewhere and executes another job.<br /><br /><strong>Timesharing or multitasking</strong> is an extension of multiprogramming.  In a timesharing system, a CPU will execute multiple jobs by switching between them, but users interacting with each program won&#8217;t notice since the jobs are switched so quickly.  This allows it so that multiple users can share one computer.<br /><br /><strong>A process</strong> is a program that is loaded into memory and is executing.  Usually, a process will only actually execute for a short time before it finishes or needs to perform I/O, like showing display to a user, or taking input from a keyboard.  A process is allocated resources like memory, and when the process terminates the OS will reclaim the resources.<br /><br /><strong>Job scheduling</strong> is when there are too many jobs to be all held in main memory; then the OS must choose which ones to execute.<br /><br /><strong>CPU scheduling</strong> is when several jobs are ready to run at the same time, and the system needs to choose which one to execute.<br /><br /><strong>The timer</strong> keeps user programs from being stuck in an infinite loop, which would prevent them from ever returning control to the OS.  The timer can be set to interrupt the computer after a specified period.  The operating system counts off the time.<br /><br /><span class="sig">Dual-Mode Operation</span><br />Operating systems must execute their own code, as well as user-defined code.  So, there are two modes of operation for each: <strong>user mode</strong>, and <strong>kernel mode</strong> (also called supervisor mode, system mode, or privileged mode).  A <strong>mode bit</strong> is added to the hardware of the computer to indicate whether the mode is presently kernel (0) or user (1).<br /><br />The hardware starts in kernel mode, then switches to user mode so that applications can be run.  If the user program makes a system call, the computer changes its mode back to kernel mode, does the proper stuff, and then returns to user mode before executing user code again.  This protects the OS from errant users, and errant users from one another.<br /><br />Since a user could accidentally overwrite OS code, most operating systems use this dual-mode thing.<br /><br /><br /><span class="sig">How does an OS manage memory?</span><br />Main memory is just a large array of bytes, which is quickly accessible by the CPU and I/O devices.  It&#8217;s usually the only memory that the CPU can address and access directly.  For a program to be executed, it must be mapped to absolute addresses and loaded into main memory; as it executes, it access instructions and data from the memory that was allocated to it, and when it terminates, the space it inhabited is declared available again.<br /><br />The OS has to keep track of who is using memory and how much, deciding which programs get moved in and out of memory, and allocating and deallocating memory space.<br /><br />Most programs are stored on a disk until their loaded into memory, and then use the disk as the source and destination of their processing.<br /><br />Note: It confused me for the longest time how &#8220;disk&#8221; is different from &#8220;memory.&#8221; I guess it&#8217;s just a CS distinction, since I know when people purchase &#8220;more memory&#8221; they&#8217;ll get, like, an external harddrive.  Hard disk?  I have a feeling I&#8217;ve been using all of these things interchangeably for forever.<br /><br /><br /><span class="sig">Special-purpose systems</span><br />Embedded computers are the most prevalent form of computers &#8211; they&#8217;re in car engines, DVDs, microwave ovens, etc.  They almost always run a <strong>real-time operating system</strong>, which is used when processors have to operate or data has to flow in a very rigidly defined amount of time.  If the processing is not done in the strict time constraint, it will fail.<br /><br /><br /><span class="sig">How does an OS manage file storage?</span><br />Storage media like magnetic disks, optical dicks, and magnetic tape have properties include access speed, capacity, data-transfer rate, and access method (sequential or random).  OS have to create and delete files (and their directories), support primitives for manipulating files and directories, map files onto secondary storage, and back up non-volatile storage media.<br /><br /><br /><br />Source:<br /><em>Operating System Concepts (8th Edition)</em>, Silberschatz, Galvin and Gagne, ISBN 978-0-470-12872-5.  Chapter 1.<br /><a href="http://en.wikipedia.org/wiki/Read-only_memory">Wikipedia: Read-only memory</a><br /><a href="http://en.wikipedia.org/wiki/Von_Neumann_architecture">Wikipedia: Von Neumann architecture</a></div>
</div>
  
  
  <div class="category-bit">Categories: 

<span class="categories">
  
    <a class='category' href='/blog/categories/compsci/'>compsci</a>, <a class='category' href='/blog/categories/notes/'>notes</a>, <a class='category' href='/blog/categories/operating-systems/'>operating systems</a>
  
</span>

</a>


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/28/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/26/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About</h1>
  I&#8217;m Nadine. I like various things.  
</section>
<section>
  <h1>Other Places</h1>
  <ul>
    <li><a href="http://uxunicorn.com">Portfolio</a></li>
    <li><a href="http://neauro.tumblr.com">Tumblr</a></li>
    <li><a href="http://github.com/neauro">Github</a></li>
    </li>
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - nadine a. -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
