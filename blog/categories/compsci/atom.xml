<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Compsci | liwanag]]></title>
  <link href="http://neauro.github.io/blog/categories/compsci/atom.xml" rel="self"/>
  <link href="http://neauro.github.io/"/>
  <updated>2014-01-08T19:24:11-08:00</updated>
  <id>http://neauro.github.io/</id>
  <author>
    <name><![CDATA[nadine a.]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Operating Systems: System Calls and I/O]]></title>
    <link href="http://neauro.github.io/blog/2011/03/04/operating-systems-system-calls-and-io/"/>
    <updated>2011-03-04T00:00:00-08:00</updated>
    <id>http://neauro.github.io/blog/2011/03/04/operating-systems-system-calls-and-io</id>
    <content type="html"><![CDATA[<div class='post'>
Okay so, applications go through the operating system to obtain data from the disk, using system calls.  There are a couple types of system calls.<br /><br />When an application makes a <strong>blocking system call</strong>, the application stops executing, and moves into the operating system's wait queue.  When the system call completes, the application gets placed back to the run queue, and when it's ready to execute, it gets the response back from the completed system call.  Most operating systems use blocking system calls because blocking application code is easier to understand.<br /><br />But some user-side processes need non-blocking I/O and so make <strong>non-blocking system calls</strong>, a good example of which are human-interface devices: i.e. the mouse, the keyboard.  Input has to be taken and used, and can't be delayed until later.  When an operating system supports non-blocking system calls, a call doesn't stop execution of the application, but rather returns quickly, indicating how much data that it was able to read immediately.<br /><br />One way that a programmer can deal with overlapping execution of code and I/O is to write a multi-threaded application, so that some threads can do blocking system calls while others execute code.<br /><br />An alternative to non-blocking system calls are <strong>asynchronous system calls</strong>, which also return immediately, and allow the application to continue executing code.  Later, when the I/O finishes, the application is notified via some shared variable or a call-back routine or an interrupt.  The main difference is that with a non-blocking system call, the immediate return contains whatever data it was able to read immediately, whereas an asynchronous system call will eventually finish reading everything it was supposed to and return that data later.</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating Systems: File Systems, Journaling File Systems]]></title>
    <link href="http://neauro.github.io/blog/2011/03/04/operating-systems-file-systems/"/>
    <updated>2011-03-04T00:00:00-08:00</updated>
    <id>http://neauro.github.io/blog/2011/03/04/operating-systems-file-systems</id>
    <content type="html"><![CDATA[<div class='post'>
It's Thursday night/Friday morning, which means it's time to cram-study for the OS quiz tomorrow!  <span class="shh">This cram session brought to you by coke obtained at a Rails party at Joshu's place.</span><br /><br /><br /><span class="sig">File Systems</span><br />So we've most recently gone over file system stuff, <a href="http://nuubu.blogspot.com/2011/02/operating-system-notes-fat-file-system.html">a little bit</a> of which I've talked about (mostly FAT file systems.)<br /><br />File system in general have a couple simple roles:<ul><li>they implement an abstraction (files/directories) for storage of data</li><li>they logically organize files (i.e. into a hierarchy of directories)</li><li>they allow people to share data (in terms of access control, consistency)</li></ul><br />A <strong>file</strong> is basically just a collection of data which also has certain properties (i.e. size, owner, last modified date, etc.) Files also have "types" which allow them to be understood by the file system or by other programs -- i.e., a file can be a .txt, or an .exe, or a directory, which is actually just a generic "file" which contains a list of other files.  Types are encoded in a file's extension.<br /><br />Some file systems allow applications to access data in different ways -- i.e. sequentially (reading bytes one at a time), or by direct access (giving the application a block/byte number), or by indexed access (like a database), etc.<br /><br />Given some path name, like <code>C:\one\two\three</code>, the file system will access that folder first by opening <code>C:</code>, then searching for <code>one\</code>, then opening <code>one\</code>, then searching for <code>two\</code>, then opening <code>two\</code>...basically the file system spends a lot of time walking down directory paths, which is why "open" is usually a separate function from "read/write."  To make directory crawling go faster, the file system will cache some prefix lookups, like <code>C:\Windows</code>.<br /><br />File systems also have the happy job of implementing a protection system for its files, either by controlling who can access a file, or by controlling by whom a file is accessed.  Some models for representing protection are<ul><li><strong>access control lists (ACLs)</strong>, in which each object knows what user can do what with it, and</li><li><strong>capabilities</strong>, in which each user knows what they can do with each object.</li></ul><br />Capabilities can be handed off, which makes sharing easier; but ACLs are easier to manage, since you can just have a file and say, "This is system32, <em>no one write to this ever</em>."  ACLs can grow pretty large when an object is heavily shared between a lot of users, though they can be simplified by categorizing the users into "groups" (i.e. an object knows it can be altered by admins, but only read by normal users).<br /><br /><br /><span class="sig">Disks</span><br />So the file system goes on top of the disk, which has all dat memorys.  Disks are always divided into five parts:<ol><li>boot block, which contains information to boot the system</li><li>superblock, which specified boundaries of next areas and contains head of freelists of inodes and file blocks</li><li>i-node area, which contains descriptors for each file on disk</li><li>file contents area, the head of which is in the super-block, and the</li><li>swap area, which holds processes which have been swapped out of memory.</li></ol><br />An i-node is a data structure traditional to a Unix-style file system, and basically stores all the information about a regular file, directory, or file system object, other than its data and name.  Each file in a file system correspondes to one i-node.  I-nodes also contain a "block list," in the form of 13 block pointers, 10 of which are "direct pointers" to a block of data corresponding to a file.  The last 3 are pointers to pointers.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-_-IcQmBgdJQ/TXCzs3P_nAI/AAAAAAAAAJU/1kKKk4i5l4E/s1600/inodes.jpg" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="174" width="320" src="http://1.bp.blogspot.com/-_-IcQmBgdJQ/TXCzs3P_nAI/AAAAAAAAAJU/1kKKk4i5l4E/s320/inodes.jpg" /></a></div><br />I-nodes and file blocks are both cached in memory, only forced to be written to disk when the command "sync" is called by the operating system (every couple seconds).  If the computer crashes or has a power failure (i.e. you get frustrated with your computer and do a hard reset), you can have an inconsistent disk.  So, avoid hard resets!<br /><br />To make sure flat file systems are consistent, ask yourself: Does each block belong precisely to one file, or else is it free?  If you're looking at the consistency of a directory structure, do all the directories form a tree, and do the number of inodes equal the number of directories you'll find if you start crawling through all of them?<br /><br /><br /><br /><span class="sig">Journaling File Systems</span><br />So in light of the fact that crashes can and probably will happen when something is being written to disk, how do you make sure stuff stays consistent?  Here is where OS takes a lot of ideas from databases, specifically, atomicity and logs.  <strong>Atomicity</strong> is when you ensure that an action/set of actions are executed completely and perfectly, or else not at all.  <strong>Logs</strong> are a record of the actions that you've done to date.<br /><br /><strong>Journaling file systems</strong> are a special (yet dated to the 80s) type of file system which takes a advantage of redo logs.  The general idea is:<ul><li>always have a "home copy" of your data in a consistent/up-to-date state</li><li>make updates persistent by writing them in order to a "journal" partition or file on disk</li><li>at your leisure, push updates to the home copies to free up space in the journal</li><li>make sure that you've written a record of your action to your log before updating the disk</li></ul><br />Once an action/transaction has been "committed" to the log, you know for sure that you want it on disk.  So, if you crash, recover your log and redo all of the actions that you did there.  This fixes the problem that you might have committed something -- i.e. intended to write it to the disk -- but ran into a problem before you were actually able to write to disk.  Redo logs are the easiest type of log to implement.  <br /><br />Once you've got a log of your "committed" data, you can have another thread walk through it and flush items to disk.  Once an item has been flushed, it can be deleted from the log.<br /><br />The problem with the log is that it's one, big contiguous write -- so though it's efficient, it is another I/O, so it's really costly performance-wise.  Thus, journaling file systems can improve performance and make recovery really efficient, but isn't worth it in a really busy system.<br /><br /><br /><br />Sources:<br />lecture 14 notes<br />Wikipedia: <a href="http://en.wikipedia.org/wiki/I-node">inode</a></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating Systems: Symbol Files, Kernel Debugging]]></title>
    <link href="http://neauro.github.io/blog/2011/02/28/operating-systems-symbol-files-kernel/"/>
    <updated>2011-02-28T00:00:00-08:00</updated>
    <id>http://neauro.github.io/blog/2011/02/28/operating-systems-symbol-files-kernel</id>
    <content type="html"><![CDATA[<div class='post'>
I'm working on the last OS project, using windbg, and the kernel keeps crashing due to something that looks like<br /><br /><pre class="brush:cpp">*** ERROR: Module load completed but symbols could not be loaded for mssmbios.sys<br />*** ERROR: Symbol file could not be found.  Defaulted to export symbols for fltmgr.sys - <br />*** ERROR: Symbol file could not be found.  Defaulted to export symbols for SHELL32.dll - <br />*** ERROR: Symbol file could not be found.  Defaulted to export symbols for USER32.dll - <br />*** ERROR: Symbol file could not be found.  Defaulted to export symbols for COMCTL32.dll - <br />*** ERROR: Symbol file could not be found.  Defaulted to export symbols for comdlg32.dll - <br />*** ERROR: Module load completed but symbols could not be loaded for NOTEPAD.EXE<br />*** ERROR: Symbol file could not be found.  Defaulted to export symbols for kernel32.dll - <br />Probably caused by : fltmgr.sys ( fltmgr!FltProcessFileLock+2049 )<br /></pre><br />I had trouble with symbol files in my previous OS project so finally decided to look them up.<br /><br />A symbol file contains debugging information, which is normally stored separate from the compiled executable to limit the size of the executable, as well as to save disk storage and reduce the time it takes to load the data.  This also makes it so that an executable can be distributed without some essential information which would make it easier for people to reverse engineer it.  Another thing that keeping debugging information separate allowed was to allow incremental linking of debug versions of programs; since the linker and integrated debugger could now use separate PDBs during debugging, the linker has less work to do.<br /><br />Concerning all modern Microsoft compilers (Visual C++ 1.0 and later), that separate file for the debugging information is called a <i>program database</i> (.pdb) file.  PDB files are created when an executable is built, given you've directed your build tools appropriately.<br /><br />In this case, the symbol file for some module couldn't be found, which means what caused the crash was third-party software that didn't have its symbols listed in a PDB.  If you had a crash, you can type <code>!analyze -v</code> in the kernel debugger window, which will show you a lot of verbose information, including the contents of the stack.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://lh6.googleusercontent.com/-kIaAvrF3BzM/TWxT24ESLPI/AAAAAAAAAI8/2OtdPaREX-M/s1600/kernelverbose.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="400" src="https://lh6.googleusercontent.com/-kIaAvrF3BzM/TWxT24ESLPI/AAAAAAAAAI8/2OtdPaREX-M/s400/kernelverbose.png" width="387" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">oh, look.&nbsp; the stack</td></tr></tbody></table><br />I've underlined the names of the modules in the stack in red.  To find out more information about a module, you can type <code>lmvs <i>moduleName</i></code>, where <code><i>moduleName</i></code> is the name of the module.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-V4DFp7WLtYE/TWxUz03FKfI/AAAAAAAAAJE/GSTiQBfaVM8/s1600/kernelverbose2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="155" src="http://3.bp.blogspot.com/-V4DFp7WLtYE/TWxUz03FKfI/AAAAAAAAAJE/GSTiQBfaVM8/s320/kernelverbose2.png" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">cool story, bro</td></tr></tbody></table><br /><br /><br />Sources:<br /><a href="http://msdn.microsoft.com/en-us/library/aa363368%28v=vs.85%29.aspx">MSDN: Symbol Files</a><br /><a href="http://support.microsoft.com/kb/121366">Microsoft Support: Description of the .PDB files and of the .DBG files</a><br /><a href="http://blogs.technet.com/b/askperf/archive/2007/05/29/basic-debugging-of-an-application-crash.aspx">TechNet: Basic Debugging of an Application Crash</a></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Notes: FAT File System]]></title>
    <link href="http://neauro.github.io/blog/2011/02/25/operating-system-notes-fat-file-system/"/>
    <updated>2011-02-25T00:00:00-08:00</updated>
    <id>http://neauro.github.io/blog/2011/02/25/operating-system-notes-fat-file-system</id>
    <content type="html"><![CDATA[<div class='post'>
<span class="sig">Disk Geometry Definitions</span><br />So now we're learning about disks (which I always end up spelling as "discs" somehow) as well as file systems.<br /><br />Some pertinent disk parts (which I always forget the definition of):<ul><li>the disk's data is stored on concentric bands called <strong>tracks</strong>; tracks are more densely populated with data at the center of the disk than on the other edges (to ensure that a <strike>discked</strike> fixed amount of data can be read in a constant time, despite disk surface speed moving fast on the outer tracks)</li><li>tracks are divided into sections called <strong>sectors</strong>, which are the smallest unit of physical storage on the disk; the data size of a sector is always a power of two, and almost always 512 bytes</li><li>a <strong>cluster</strong> is what the Windows NT file systems use to allocate storage; a cluster is one or more contiguous sectors.</li></ul><br />As a file is written to the disk, the file system decides the appropriate number of clusters needed to store the file's data.  For instance, if each cluster is 512 bytes and the file being written is 800 bytes, then two clusters are allocated for the file.  If the file size changes later, then more clusters are allocated to that file.<br /><br /><br /><br /><span class="sig">One type of file system: File Allocation Table (FAT)</span><br />So, a file system is a method of storing and organizing files and data on a computer.  Essentially, a file system organizes the files into a database so that the operating system can organize, manipulate, and retrieve them.<br /><br />Some basic differences (between FAT and NTFS file systems):<ul><li>The FAT file system can only use 16 bits for the cluster number, which means that volumes using the FAT format of file system can be larger than 65,535 sectors</li><li>Because of the overhead in the FAT file system, it's not good for volumes larger than 511 MB (wait, isn't this...all volumes nowadays?;;)</li><li>FAT is a better choice than NTFS for volumes that are smaller than ~400-500 MBs though, because of the disk overhead in NTFS.  I guess that's the tradeoff</li><li>FAT file systems can be used with operating systems other than Windows NT, i.e. Windows 95, MS-DOS, etc.</li><li>FAT is simpler</li><li>FAT folder size is smaller for an equal number of files</li><li>FAT has no controls regarding whether a user can access a file or folder, which means the system doesn't need to check permissions for an individual file or user, to see if the user has access to the file.  But Windows NT operating systems will have to check if a file is read-only anyway, whether it's FAT or NTFS, which is why FAT may provide faster access to files in this respect</li></ul><br />FAT is a simple file system that was originally designed for small disks and simple folder structures. A FAT-formatted volume is allocated in clusters, the size of which is determined by the size of the volume, which must fit in 16 bits and must be a power of 2.<br /><br />FAT is named for the file allocation table, which lives at the beginning of a FAT-formatted volume.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-jw8lKfAxw60/TWX-0tUJQVI/AAAAAAAAAIs/9T9URTfof_c/s1600/FATvolume.jpg" imageanchor="1" style="margin-left:1em; margin-right:1em"><img border="0" height="62" width="320" src="http://3.bp.blogspot.com/-jw8lKfAxw60/TWX-0tUJQVI/AAAAAAAAAIs/9T9URTfof_c/s320/FATvolume.jpg" /></a></div><br />What do all of these areas do?  <strong>FAT1</strong> and <strong>FAT2</strong> contain the file allocation table, which contains information about a cluster on the volume, specifically whether that cluster is:<ul><li>unused</li><li>a cluster in use by a file</li><li>a bad cluster</li><li>the last cluster in a file</li></ul><br />The <strong>root folder</strong> contains an entry for each file and folder on the root.  The only difference between the root folder and other folders is that the root folder is in a specified location on the disk, and has a fixed size.  (On a hard disk, the size is 512 entries; on a floppy disk, it depends on the size of the floppy.)<br /><br />For every file and folder contained in a folder (as directories are really just a special type of file), there is a 32-byte entry, which contains the following information:<ul><li>name (eight-plus-three characters)</li><li>attribute byte (8 bits)</li><li>create time (24 bits)</li><li>create date (16 bits)</li><li>last access date (16 bits)</li><li>last modified time (16 bits)</li><li>last modified date (16 bits)</li><li>starting cluster number in the file allocation table (16 bits)</li><li>file size (32 bits)</li></ul><br />As for the FAT folder structure, there is no real organization -- when a new file comes in, it is placed in the first available location on the volume.  The "<strong>starting cluster number</strong>" is the address of the first cluster used by the file.  Each cluster points to the next cluster used in the file, or else to a terminating cluster that signals the end of the file.<br /><br />Since all entries in a folder are the same size, the <strong>attribute byte</strong> for each entry in a folder describes what kind of entry it is: one bit indicating that it's a subfolder, another bit marking the entry as a volume label, etc.  The OS controls the settings of these bits.  The user can control four other attribute bits, which indicate the entry is an archive file, system file, hidden file, and/or read-only file.<br /><br /><br /><br />More later.<br /><br /><br />Sources:<br />Microsoft TechNet: <a href="http://technet.microsoft.com/en-us/library/cc750198.aspx">Disk and File System Basics</a>, <a href="http://technet.microsoft.com/en-us/library/cc750355.aspx">Choosing a File System</a><br />Wikipedia: <a href="http://en.wikipedia.org/wiki/File_system">File system</a></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating Systems Notes [Disk Caching, RAID Levels, Flash Memory)]]></title>
    <link href="http://neauro.github.io/blog/2011/02/17/operating-systems-notes-disk-caching/"/>
    <updated>2011-02-17T00:00:00-08:00</updated>
    <id>http://neauro.github.io/blog/2011/02/17/operating-systems-notes-disk-caching</id>
    <content type="html"><![CDATA[<div class='post'>
This is a quick rehash of what we've learned in the previous week!<br /><br /><span class="sig">Disk Caching</span><br />So the OS can optimize operations done by scheduling things, either <a href="http://nuubu.blogspot.com/2011/01/operating-systems-notes-chapter-5-cpu.html">CPU</a> or <a href="http://nuubu.blogspot.com/2011/02/operating-systems-notes-memory.html">memory</a>.  The operating system can also schedule disk operations, which are reads/writes from/to the disk (that far-off entity which stores the data you want, yet is time-consuming and expensive to reach).<br /><br /><strong>Disk scheduling</strong> is optimized especially through <strong>disk caching</strong>.  Disk caching can come in two flavors.  <strong>Write caching</strong> is when the user has the data that they want to write to disk, and the OS, rather than writing that data directly to the disk, instead writes it to a cache.  Then, sometime later, the OS writes everything in the cache to the disk.  There's a sort of "five minute rule" to this, where (very roughly) every "five minutes or so," the cache gets "flushed" to disk.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-wDujboA5CPU/TV4Ffh_w4XI/AAAAAAAAAHs/qyHyYUHJcPs/s1600/writecaching.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="81" src="http://3.bp.blogspot.com/-wDujboA5CPU/TV4Ffh_w4XI/AAAAAAAAAHs/qyHyYUHJcPs/s320/writecaching.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">our TA's lecture notes are so cool</td></tr></tbody></table><br />There's also <strong>read caching</strong>, which is when the operating system reads data from the disk into a cache, so as to be easily and quickly accessed by the user later.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-6A1WwUsxBH8/TV4FjpY-ETI/AAAAAAAAAH0/UahzFruaJtY/s1600/readcaching.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="83" src="http://1.bp.blogspot.com/-6A1WwUsxBH8/TV4FjpY-ETI/AAAAAAAAAH0/UahzFruaJtY/s320/readcaching.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">look at this. &nbsp;just look how understandable this is</td></tr></tbody></table><br /><br />One way to take advantage of disk caching is to use <strong>memory mapped I/O</strong>.  The data in a disk cache exists in kernel space, so what's an easy way for a process in user-space to get a hold of it?  To have the process contain a mapping directly to that disk cache!<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-OU-tACxsM5Q/TV4GYm7X0jI/AAAAAAAAAH8/BQaW5asqgME/s1600/memorymappedio.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="126" src="http://2.bp.blogspot.com/-OU-tACxsM5Q/TV4GYm7X0jI/AAAAAAAAAH8/BQaW5asqgME/s320/memorymappedio.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">these colors are just so neat</td></tr></tbody></table><br />The segment of a process's <a href="http://nuubu.blogspot.com/2011/02/operating-systems-notes-memory.html">virtual address</a> which has been mapped is called a <strong>memory-mapped file</strong>; in this case, it's the blue part.<br /><br />As the process uses the file, it may need to write to it, which means eventually that the pages that the user-space process altered will need to be flushed to disk, or written out to disk.  The OS will know which pages to write thanks to the alteration of a dirty bit.  When the ditry bit is high, that means that the page has been altered and will need to be flushed.  <span class="shh">(Somehow this whole paragraph feels a little gross...)</span><br /><br /><br /><span class="sig">RAID Levels</span><br />are another thing that we've been talking about in lecture.  The thing about disks in general is that though they are a pretty useful/necessary storage device for computers, they're super messy -- they run into errors all the time, like bad blocks, or missed seeks.  Part of the job of the operating system is to hide all this messy-ness from higher-level software (i.e. software that other, more sane programmers will write to be run on the computer).  The OS will provide different levels of access to different clients -- for instance, allowing them to access physical disk blocks (a specific head, or cylinder of the disk) or just logical disk blocks (i.e. a program says "I want disk 6" and the OS retrieves the real location of "disk 6" itself).<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-pZ8QGur6iSg/TV4MPA-H3hI/AAAAAAAAAIE/Sv47rFhynK0/s1600/harddrive.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="223" src="http://4.bp.blogspot.com/-pZ8QGur6iSg/TV4MPA-H3hI/AAAAAAAAAIE/Sv47rFhynK0/s320/harddrive.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">this is what a disk looks like, by the way</td></tr></tbody></table><br />It takes time for an OS to find something on a disk.  The performance is based on three main things:<br /><ol><li><strong>Seeking</strong>: how long it takes to move the disk arm to the correct cylinder on the disk</li><li><strong>Rotation</strong>: how long it takes to wait for the sector you want to rotate under the head of the reader.  This depends on the rotation rate of the disk, which is not increasing that quickly, mainly because the way disks are made now, if they spin any faster they'll fall apart</li><li><strong>Transfer</strong>: how long it takes to transfer data from the surface of the disk into the disk controller, and from there back to the process that wants the data</li></ol><br />The OS mainly tries to reduce seek times and rotation.<br /><br />So, since disk transfer rates in general are improving, and since CPU performance isn't, what you can do to improve performance is have multiple disks containing/transferring data.  In particular, you can "stripe" files across multiple disks by placing parts of each file on a different disk, making it so that you can read parts of a file simultaneously (i.e. use <strong>parallel I/O</strong> to read a single file).<br /><br />The problem with striping is that it's not very reliable.  To improve its reliability, you can add redundant data to the disks, along with striping, and thus -- <strong>RAID: Redundant Array of Inexpensive Disks</strong>.  Since disks are physically small and cheap, you can just stick a lot of them into one box to increase storage, performance, and availability.  Depending on how you stripe data, you can affect resulting performance and reliability in different ways.<br /><br />What are some things to consider in different methods of striping?  Firstly, <strong>granularity</strong>.  If you stripe each file across multiple disks ("fine-grained" granularity), you will get high throughput for reading the file, but limit overall transfer to one file at a time.  If you stripe each file over only a few disks ("coarse-grained" granularity), you limit throughput for 1 file, but can have concurrent access to multiple files.<br /><br />Another thing to think about is the <strong>redundancy</strong> itself.  If you uniformly distribute redundancy, then you'll avoid load-balancing problems.  You can also concentrate redundancy information on a subset of your total disks, and make it so that some of your total disks are "data disks" and the others are "redundancy disks."<br /><br />So what are the different types of RAID?<br /><br /><strong>RAID Level 0</strong> is when you have a non-redundant disk array -- files are just striped across disks, with no redundant info saved at all.  There's high read throughput (since you can do the parallel I/O thing), and also high write throughput (since you don't need to write redundant info).  However, if one of your disks fails, then you can lose the file and sometimes the entire volume, which is horrible. D: D:<br /><br /><strong>RAID Level 1</strong> is also known as "mirrored disks." Files are striped across half the disks, and when you write data, it gets written to both a "data disk" and the "mirror disks," which are copies of the data disk.  This way, if the data disks fail, you can just use the surviving disks.  A downside to this technique is that you need to have 2N the amount of space to hold N amount of data, understandably.<br /><br /><strong>RAID Level 2, 3, 4</strong> all pretty much have the same idea, I guess -- they all use error correcting code (ECC) or <strong>parity disks</strong> to provide fault tolerance.  Each byte on a parity disks is a parity function of the corresponding bytes on all the other disks.  This way, a read accesses all data disks.  A write accesses all data disks, as well as the parity disk.  If the disk fails, read what you have of the file and then use the parity disk to compute the missing data.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-hxvTkkUW3Bs/TV4N-F5JUiI/AAAAAAAAAIM/CHZ7y8sofE0/s1600/parity%2Bdrives.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="105" src="http://2.bp.blogspot.com/-hxvTkkUW3Bs/TV4N-F5JUiI/AAAAAAAAAIM/CHZ7y8sofE0/s320/parity%2Bdrives.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">yeah! &nbsp;go parity drive, go!</td></tr></tbody></table><br />But, <strong>what is parity, again?</strong> I guess, given a byte, you want to add a bit set so that the total number of 1's in the byte is even; that way, any single missing bit can be reconstructed.  Parity bites are used as the simplest form of error detecting.<br /><br /><strong>RAID Level 5</strong> uses <strong>block interleaved distributed parity</strong>, which is just like the parity scheme of earlier RAIDs, but instead of having one parity disk, the parity info is distributed across all disks.  For each block, one disk holds the parity, and the other disks hold the data.  This type of RAID has significantly better performance, since the parity disk is not a hot spot.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-MwfHA3qWfcQ/TV4O_26wZlI/AAAAAAAAAIU/FyZ1f6cnCsA/s1600/raid5.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="150" src="http://4.bp.blogspot.com/-MwfHA3qWfcQ/TV4O_26wZlI/AAAAAAAAAIU/FyZ1f6cnCsA/s320/raid5.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">RAID Level 5 disk layout</td></tr></tbody></table><br />So when you use RAID, you can also use a <strong>RAID controller</strong>, which is embedded in the hardware.  A RAID controller can make it seem to the OS like the many disks you're using is actually just one disk.<br /><br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-gvTCVSUmMAU/TV4PdUh6pbI/AAAAAAAAAIc/qiESnTQW4-M/s1600/raidcontroller.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="135" src="http://4.bp.blogspot.com/-gvTCVSUmMAU/TV4PdUh6pbI/AAAAAAAAAIc/qiESnTQW4-M/s320/raidcontroller.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">"What's that? &nbsp;I only have one disk? &nbsp;Oh, cool"</td></tr></tbody></table><br />The advantages of putting <em>policy</em> in the RAID controller is that it makes the OS's job easier.  The disadvantage is that every time you put a layer of abstraction into something, you ruin the chance of optimizing it.  So, putting this layer of abstraction makes the OS a little simpler to program, but now you can't optimize the OS (like, using disk scheduling effectively) the way you could if the OS <em>did</em> have control of and was aware of all of its assets.<br /><br /><br /><span class="sig">Solid State Disks (Flash)</span><br />We also talked about flash drives, which was pretty interesting.  Like disks, the data on a flash drive is stored in blocks.  Since there aren't any spinning platters, random access is fast, and you don't need disk scheduling algorithms.  It's very fast to read from a flash drive thanks to the random access, but slower to write to a flash drive, because the flash media must be erased before it can be written to.  So, when you're writing to a solid state drive like a flash drive, you're actually a) reading some block, b) erasing that block, c) writing back the modified block.  But, there are ways of "hiding the warts of an SSD" (as these section notes put it), mostly by virtualizing pages and blocks on the drive (so you're working with logical pages, not physical pages), and doing "wear-leveling," which is when you try to spread our the erasure of blocks evenly over the drive.<br /><br />Meanwhile, what is a solid-state drive?  Evidently it's just a data storage device toat uses solid-state memory to store persistent data.  "Solid-state" refers to a type of electronics which are build entirely from solid materials, within which electrons or other charge carriers are confined within the solid material.  Solid-state memory I think is like RAM, which gets erased once you shut down your computer, but as of 2010, most solid-state drives use NAND-based flash memory, which can hold memory even if there's no power running through it.  "NAND flash" just refers to the way that the transistors are hooked up in the flash drive: in a way that resembles a NAND gate.<br /><br />Sources:<br />section 7 notes<br /><a href="http://en.wikipedia.org/wiki/Memory-mapped_file">Wikipedia: Memory-mapped file</a>, <a href="http://en.wikipedia.org/wiki/Parity_disk">Parity Drive</a>, <a href="http://en.wikipedia.org/wiki/Parity_bit">Parity bit</a>, <a href="http://en.wikipedia.org/wiki/Solid_state_drive">Solid-state drive</a>, <a href="http://en.wikipedia.org/wiki/Solid-state_(electronics)">Solid state (electronics)</a>, <a href="http://en.wikipedia.org/wiki/Flash_memory#NAND_flash">NAND flash</a><br />lecture 12 notes ("Disk")<br />lecture 13 notes ("Raid and Volumes")</div>

]]></content>
  </entry>
  
</feed>
